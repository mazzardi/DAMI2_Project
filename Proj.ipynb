{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Table of Contents\n",
    "* [1 PREPROCESSING](#chapter1)\n",
    "    * [1.1 Describe](#section_1_1)\n",
    "    * [1.2 Handle missing values](#section_1_2)\n",
    "    * [1.4 Split](#section_1_3)\n",
    "* [2 TRAINING](#chapter2)\n",
    "    * [2.1 Variables](#section_2_1)\n",
    "    * [2.2 Functions](#section_2_2)\n",
    "    * [2.3 Training](#section_2_3)\n",
    "* [3 RESULTS](#chapter3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "______________\n",
    "Importing necessary libraries and dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from models import UnbalancedBagger, UnbalancedBaggerNoSampling\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import recall_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import operator\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "RANDOM_SEED = 12345"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"datasets/diabetes.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PREPROCESSING <a class=\"anchor\" id=\"chapter1\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Describe <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "* Understand dataset by looking at feature set\n",
    "* Show class imbalance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_labels = original_df.Outcome.value_counts()\n",
    "x_labels = [0,1]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = total_labels.plot(kind='bar')\n",
    "ax.set_title('Class label distribution (Amount)')\n",
    "ax.set_xlabel('Labels')\n",
    "ax.set_ylabel('Amount')\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "for rect,lbl in zip(rects, total_labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height\n",
    "            , lbl, ha='center', va='bottom')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_labels = [0,1]\n",
    "total_labels_percentage = original_df.Outcome.value_counts()/len(original_df)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = total_labels_percentage.plot(kind='bar')\n",
    "ax.set_title('Class label distribution (%)')\n",
    "ax.set_xlabel('Labels')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "rects = ax.patches\n",
    "labels = []\n",
    "\n",
    "for lbl in total_labels_percentage:\n",
    "    labels.append(\"{0:.2f}\".format(lbl))\n",
    "\n",
    "for rect,lbl in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height\n",
    "            , lbl, ha='center', va='bottom')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handle missing values <a class=\"anchor\" id=\"section_1_2\"></a>\n",
    "* Find missing values\n",
    "* Imputate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Amount of 0 values:\")\n",
    "print(\"Glucose: \", len(original_df[original_df[\"Glucose\"] == 0]))\n",
    "print(\"BloodPressure: \", len(original_df[original_df[\"BloodPressure\"] == 0]))\n",
    "print(\"SkinThickness: \", len(original_df[original_df[\"SkinThickness\"] == 0]))\n",
    "print(\"Insulin: \", len(original_df[original_df[\"Insulin\"] == 0]))\n",
    "print(\"BMI: \", len(original_df[original_df[\"BMI\"] == 0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def imputation(data, columns):\n",
    "    \"\"\"\n",
    "    Imputates input data with mean values of corresponding columns.\n",
    "\n",
    "    Keyword arguments:\n",
    "    data -- Dataset to impute.\n",
    "    columns -- Which columns to compute.\n",
    "\n",
    "    returns imputed dataset\n",
    "    \"\"\"\n",
    "    for c in columns:\n",
    "         if c in data.columns:\n",
    "             data[c].replace(0, np.nan,inplace=True)\n",
    "             data[c].fillna(data[c].mean(), inplace=True)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_df = imputation(original_df, [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"])\n",
    "\n",
    "original_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split <a class=\"anchor\" id=\"section_1_3\"></a>\n",
    "* Split dataset into examples and labels.\n",
    "* Standardize examples for SVC models\n",
    "* train_test_split for future model training, keeping class imbalance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#we can now split attributes and class label\n",
    "X = original_df.drop('Outcome', axis=1)\n",
    "y = original_df.Outcome\n",
    "\n",
    "X.describe()\n",
    "\n",
    "#data_standardized used for SVC\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(X)\n",
    "data_standardized = pd.DataFrame(data_standardized,columns=X.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test, = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "#standardized split for SVC\n",
    "X_train_svc,X_test_svc,y_train_svc,y_test_svc, = train_test_split(data_standardized, y, test_size=0.3, random_state=RANDOM_SEED,stratify=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_labels = [0,1]\n",
    "total_labels_percentage = y_train.value_counts()/len(y_train)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = total_labels_percentage.plot(kind='bar')\n",
    "ax.set_title('Class label distribution (%)')\n",
    "ax.set_xlabel('Labels')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "rects = ax.patches\n",
    "labels = []\n",
    "\n",
    "for lbl in total_labels_percentage:\n",
    "    labels.append(\"{0:.2f}\".format(lbl))\n",
    "\n",
    "for rect,lbl in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height\n",
    "            , lbl, ha='center', va='bottom')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRAINING <a class=\"anchor\" id=\"chapter2\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variables <a class=\"anchor\" id=\"section_2_1\"></a>\n",
    "* Define and declare needed variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid_svc = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf', 'poly']},\n",
    " ]\n",
    "\n",
    "param_grid_dt = [\n",
    "    {'splitter' : ['best', 'random'],\n",
    "     'min_samples_split':[2,5,10],\n",
    "     'min_samples_leaf':[1,5,10],\n",
    "     'min_weight_fraction_leaf':[0,0.05, 0.1, 0.15, 0.2]\n",
    "     }\n",
    "]\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=RANDOM_SEED)\n",
    "svc = SVC(random_state=RANDOM_SEED, probability=True)\n",
    "\n",
    "ros = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "rus = RandomUnderSampler(random_state=RANDOM_SEED)\n",
    "smote = SMOTE(random_state=RANDOM_SEED, sampling_strategy='minority')\n",
    "\n",
    "search_dtc = GridSearchCV(dtc, param_grid_dt, cv=10, scoring='average_precision')\n",
    "search_svc = GridSearchCV(svc, param_grid_svc, cv=10, scoring='average_precision')\n",
    "\n",
    "list_of_auc = []\n",
    "list_of_auprc = []\n",
    "list_of_avg_prec = []\n",
    "list_of_mcnemar_results = []\n",
    "list_of_recall_results = []\n",
    "list_of_precision_results = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions <a class=\"anchor\" id=\"section_2_2\"></a>\n",
    "* Functions needed for training and results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def smote_technique(X,y):\n",
    "    \"\"\"Samples X using SMOTE.\n",
    "\n",
    "    Keyword arguments:\n",
    "    X -- Examples of original dataset\n",
    "    y -- True labels of examples\n",
    "\n",
    "    Returns:\n",
    "    Sampled result\n",
    "\n",
    "    Further reading:\n",
    "    https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "    \"\"\"\n",
    "    x_sample, y_sample = smote.fit_sample(X, y)\n",
    "    return x_sample,y_sample\n",
    "\n",
    "def random_oversampling_technique(X,y):\n",
    "    \"\"\"Samples X using Random Oversampling.\n",
    "\n",
    "    Keyword arguments:\n",
    "    X -- Examples of original dataset\n",
    "    y -- True labels of examples\n",
    "\n",
    "    Returns:\n",
    "    Sampled result\n",
    "\n",
    "    Further reading:\n",
    "    https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.RandomOverSampler.html\n",
    "    \"\"\"\n",
    "    x_sample, y_sample = ros.fit_sample(X, y)\n",
    "    return x_sample,y_sample\n",
    "\n",
    "def random_undersampling_technique(X,y):\n",
    "    \"\"\"Samples X using Random Undersampling.\n",
    "\n",
    "    Keyword arguments:\n",
    "    X -- Examples of original dataset\n",
    "    y -- True labels of examples\n",
    "\n",
    "    Returns:\n",
    "    Sampled result\n",
    "\n",
    "    Further reading:\n",
    "    https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html\n",
    "    \"\"\"\n",
    "    x_sample, y_sample = rus.fit_sample(X, y)\n",
    "    return x_sample,y_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_AUPRC_curves(list_of_rpl):\n",
    "    \"\"\"\n",
    "    Creates AUPRC curves using Pyplot.\n",
    "\n",
    "    Keyword arguments:\n",
    "    list_of_rpl -- List of precision-recall pairs for different probability thresholds on the positive class, for each model.\n",
    "    \"\"\"\n",
    "    #Create subplots on one row and two columns:\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\n",
    "    #Set labels on subplot 0:\n",
    "    axes[0].set(xlabel=\"recall\")\n",
    "    axes[0].set(ylabel=\"precision\")\n",
    "    axes[0].set(title=\"DT\")\n",
    "    #Set labels on subplot 1:\n",
    "    axes[1].set(xlabel=\"recall\")\n",
    "    axes[1].set(ylabel=\"precision\")\n",
    "    axes[1].set(title=\"SVC\")\n",
    "    #Iterate each curve and plot to either subplot 0 or subplot 1, depending on model used:\n",
    "    for recall, prec, lbl in list_of_rpl:\n",
    "        #If lbl contains substring \"DT\" then that curve belongs to subplot 0:\n",
    "        if \"DT\" in lbl:\n",
    "            axes[0].plot(recall,prec,marker='.', label=lbl)\n",
    "        #Else a SVC model was used and belongs to subplot 1:\n",
    "        else:\n",
    "            axes[1].plot(recall,prec,marker='.', label=lbl)\n",
    "    #Thighten the figure:\n",
    "    fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_AUC_values(y_preds, y_true, label):\n",
    "    \"\"\"\n",
    "    Calculate AUC value using SKlearn roc_auc_score.\n",
    "\n",
    "    Keyword arguments:\n",
    "    y_preds -- Predicted labels using a model.\n",
    "    y_true -- True labels.\n",
    "    label -- Label used to distinguish model from other models.\n",
    "\n",
    "    Returns:\n",
    "    Returns AUC score followed by the label parameter as a list.\n",
    "\n",
    "    Further reading:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "    \"\"\"\n",
    "    score = roc_auc_score(y_true,y_preds)\n",
    "    return [score, label]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_AUPRC_values(y_probs, y_true, label):\n",
    "    \"\"\"\n",
    "    Calculate average precision and AUPRC curves.\n",
    "\n",
    "    Keyword arguments:\n",
    "    y_probs -- Predicted class probabilites using a model.\n",
    "    y_true -- True labels.\n",
    "    label -- Label used to distinguish model from other models.\n",
    "\n",
    "    Returns:\n",
    "    (1) Returns AUPRC curve followed by the label parameter as a list.\n",
    "    (2) Returns average precision score followed by the label parameter as a list.\n",
    "\n",
    "    Further reading:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n",
    "    \"\"\"\n",
    "    pos_probs = y_probs[:,1]\n",
    "    prec,recall,_ = precision_recall_curve(y_true, pos_probs)\n",
    "    avg_prec = average_precision_score(y_true, pos_probs)\n",
    "    return [prec,recall,label], [avg_prec, label]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_cont_table(model_one_preds, model_two_preds, true_preds):\n",
    "    \"\"\"\n",
    "    Calculate contingency table for McNemar test.\n",
    "\n",
    "    Keyword arguments:\n",
    "    model_one_preds -- Predictions of model one.\n",
    "    model_two_preds -- Predictions of model two.\n",
    "    true_preds -- True labels.\n",
    "\n",
    "    Returns:\n",
    "    Contingency table as: [[both models correct, model one correct], [model two correct, none of the models correct]].\n",
    "    \"\"\"\n",
    "    #Init table variables\n",
    "    both_correct, model1_correct, model2_correct, non_correct = 0,0,0,0\n",
    "    #Iterate 0-length of ndarray true_preds\n",
    "    for idx in range(len(true_preds)):\n",
    "        #if predicted label of model  one is the same as the true label\n",
    "        if model_one_preds[idx] == true_preds[idx]:\n",
    "            #if predicted label of model two is the same as the true label\n",
    "            if model_two_preds[idx] == true_preds[idx]:\n",
    "                both_correct = both_correct + 1\n",
    "            #Only model one predicted the true label\n",
    "            else:\n",
    "                model1_correct = model1_correct + 1\n",
    "        #Else if predicted label of model two is the same as the true label\n",
    "        elif model_two_preds[idx] == true_preds[idx]:\n",
    "            model2_correct = model2_correct + 1\n",
    "        #Else, none of the models predicted the true label\n",
    "        else:\n",
    "            non_correct = non_correct + 1\n",
    "    return [[both_correct,model1_correct],[model2_correct,non_correct]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_mcneamar(table):\n",
    "    \"\"\"\n",
    "    Calculate McNemar result using statsmodel mcnemar.\n",
    "\n",
    "    Keyword arguments:\n",
    "    table -- Contingency table of two models (See function: calculate_cont_table).\n",
    "\n",
    "    Returns:\n",
    "    (1) Statistic's result\n",
    "    (2) p-value\n",
    "\n",
    "    Further reading:\n",
    "    https://www.statsmodels.org/dev/generated/statsmodels.stats.contingency_tables.mcnemar.html\n",
    "    \"\"\"\n",
    "    result = mcnemar(table, exact=False, correction=False)\n",
    "    return result.statistic, result.pvalue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_mcnemar_df():\n",
    "    \"\"\"\n",
    "    Creates and returns a dataframe of McNemar results.\n",
    "\n",
    "    Returns:\n",
    "    Dataframe of McNemar results.\n",
    "    \"\"\"\n",
    "    #Create emtpy lists to hold values\n",
    "    labels = []\n",
    "    p_values = []\n",
    "    stats = []\n",
    "    #Iterate list_of_mcnemar_results containing the McNemar results of each model\n",
    "    for label, values in list_of_mcnemar_results:\n",
    "        #Append iter.items to corresponding list\n",
    "        labels.append(label)\n",
    "        stats.append(values[0])\n",
    "        p_values.append(values[1])\n",
    "    #Create dict of key-value pairs\n",
    "    data = {'Model':labels, 'Statistic':stats, 'p_value':p_values}\n",
    "    #Create dataframe using dictionary\n",
    "    df = pd.DataFrame(data,columns=[\"Model\", \"Statistic\", \"p_value\"])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_preds_and_probs(classifier, X_test):\n",
    "    \"\"\"\n",
    "    Gets classifier predictions and predicted probabilites using SKlearn predict(X) and predict_proba(X).\n",
    "\n",
    "    Keyword arguments:\n",
    "    classifier -- The classifier to use for predicting and predicting probabilites.\n",
    "    X_test -- The set of examples to predict.\n",
    "\n",
    "    Returns:\n",
    "    Classifier predictions and predicted probabilites.\n",
    "    \"\"\"\n",
    "    preds = classifier.predict(X_test)\n",
    "    probs = classifier.predict_proba(X_test)\n",
    "    return preds,probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_and_add_classifier_results(class_preds, class_probs, y_true, label):\n",
    "    \"\"\"\n",
    "    Calculates recall, precision, AUC and AUPRC and adds them to their corresponding lists.\n",
    "\n",
    "    Keyword arguments:\n",
    "    class_preds -- Predicted labels of used model.\n",
    "    class_probs -- Predicted probabilites of used model.\n",
    "    y_true -- True labels.\n",
    "    label -- Label used to distinguish model from other models.\n",
    "\n",
    "    Further reading:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "    \"\"\"\n",
    "    recall = recall_score(y_true, class_preds, average='binary',pos_label=1)\n",
    "    list_of_recall_results.append([recall, label])\n",
    "\n",
    "    precision = precision_score(y_true, class_preds, average='binary',pos_label=1)\n",
    "    list_of_precision_results.append([precision, label])\n",
    "\n",
    "    list_of_auc.append(get_AUC_values(class_preds,y_true, label))\n",
    "\n",
    "    auprc, avg_prec = get_AUPRC_values(class_probs, y_true, label)\n",
    "    list_of_auprc.append(auprc)\n",
    "    list_of_avg_prec.append(avg_prec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_and_add_significance(classifier_one, classifier_two, y_true, label):\n",
    "    \"\"\"\n",
    "    Calculates McNemar significance of two input models and store result in list_of_mcnemar_results.\n",
    "\n",
    "    Keyword arguments:\n",
    "    classifier_one -- Prediction results of first classifier used.\n",
    "    classifier_two -- Prediction results of second classifier used.\n",
    "    y_true -- True labels.\n",
    "    label -- Label used to distinguish model from other models.\n",
    "    \"\"\"\n",
    "    table = calculate_cont_table(classifier_one, classifier_two, y_true.values)\n",
    "    list_of_mcnemar_results.append([label, calculate_mcneamar(table)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_SVC_DT(list):\n",
    "    \"\"\"\n",
    "    Split input list depending on substring 'DT' and 'SVC'.\n",
    "\n",
    "    Keyword arguments:\n",
    "    list -- Input list to split.\n",
    "\n",
    "    Returns:\n",
    "    (1) A list containing the DT models.\n",
    "    (2) A list containing the SVC models.\n",
    "    \"\"\"\n",
    "    #Creates empty list of DT and SVC models.\n",
    "    DT_list = []\n",
    "    SVC_list = []\n",
    "    #Iterate input list values\n",
    "    for score,item in list:\n",
    "        #If sub-item contains substring 'DT' then belongs to DT_list, which contains the DT models\n",
    "        if 'DT' in item:\n",
    "            DT_list.append([score,item])\n",
    "        #Else belongs to SVC_list, which contains the SVC models\n",
    "        else:\n",
    "            SVC_list.append([score,item])\n",
    "    return DT_list, SVC_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sort(list):\n",
    "    \"\"\"\n",
    "    Sorts input-list according to value of item on index 0 per sub-item.\n",
    "\n",
    "    Keyword arguments:\n",
    "    list -- Input list to sort.\n",
    "\n",
    "    Returns:\n",
    "    A sorted list of the original list.\n",
    "    \"\"\"\n",
    "    sorted_list = sorted(list, key=operator.itemgetter(0), reverse=True)\n",
    "    return sorted_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_bars(list, title):\n",
    "    \"\"\"\n",
    "    Creates a barh which correspond to input-list.\n",
    "\n",
    "    Keyword arguments:\n",
    "    list -- Input list to plot.\n",
    "    title -- Title for the barh.\n",
    "\n",
    "    Further reading:\n",
    "    https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "    \"\"\"\n",
    "    #unzip sub-items to list of labels and list of values.\n",
    "    y,x = zip(*list)\n",
    "    #create new figure of figsize=12,8\n",
    "    plt.figure(figsize=(12,8))\n",
    "    #plot barh with x,y values\n",
    "    plt.barh(x, y)\n",
    "    #plot title\n",
    "    plt.title(title)\n",
    "    #Iterate values and plot them next to corresponding bar\n",
    "    for index, value in enumerate(y):\n",
    "        plt.text(value, index, str(\"{0:.2f}\".format(value)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_bars(list, label):\n",
    "    \"\"\"\n",
    "    Splits, sorts and creates barh plots of input-list.\n",
    "\n",
    "    Keyword arguments:\n",
    "    list -- List to split, sort and plot.\n",
    "    label -- Title of barh.\n",
    "    \"\"\"\n",
    "    #split input-list to dt_models and svc_models\n",
    "    dt_models, svc_models = split_SVC_DT(list)\n",
    "    #sort the new lists\n",
    "    sorted_dt = sort(dt_models)\n",
    "    sorted_svc = sort(svc_models)\n",
    "    #plot bars of the lists\n",
    "    create_bars(sorted_dt, label + \", DT\")\n",
    "    create_bars(sorted_svc, label + \", SVC\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training <a class=\"anchor\" id=\"section_2_3\"></a>\n",
    "* Train models and store results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#new method, using smote, ros, rus\n",
    "sampling_techniques = [smote, ros, rus]\n",
    "svc = SVC(C=1000, gamma=0.001, random_state=12345, probability=True)\n",
    "dtc = DecisionTreeClassifier(min_samples_leaf=10, min_weight_fraction_leaf=0,random_state=12345)\n",
    "\n",
    "dt_predict, dt_proba = UnbalancedBagger.unbalanced_bagger(X_train, y_train, 10, sampling_techniques,base_estimator=dtc,random_seed=RANDOM_SEED)\n",
    "svc_predict, svc_proba = UnbalancedBagger.unbalanced_bagger(X_train_svc, y_train_svc, 10, sampling_techniques,base_estimator=svc,random_seed=RANDOM_SEED)\n",
    "\n",
    "predictions_dt_unb = dt_predict(X_test)\n",
    "dt_probs_preds = dt_proba(X_test)\n",
    "predictions_svc_unb = svc_predict(X_test_svc)\n",
    "svc_probs_preds = svc_proba(X_test_svc)\n",
    "\n",
    "calc_and_add_classifier_results(predictions_dt_unb, dt_probs_preds, y_test, \"Unb_bagging, DT\")\n",
    "calc_and_add_classifier_results(predictions_svc_unb, svc_probs_preds, y_test_svc, \"Unb_bagging, SVC\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train on original data\n",
    "dt_classifier = search_dtc.fit(X_train, y_train).best_estimator_\n",
    "svc_classifier = search_svc.fit(X_train_svc, y_train_svc).best_estimator_\n",
    "\n",
    "dt_predictions, dt_probs_preds = get_preds_and_probs(dt_classifier, X_test)\n",
    "svc_predictions, svc_probs_preds = get_preds_and_probs(svc_classifier, X_test_svc)\n",
    "\n",
    "calc_and_add_classifier_results(dt_predictions, dt_probs_preds, y_test, \"Original, DT\")\n",
    "calc_and_add_classifier_results(svc_predictions, svc_probs_preds, y_test_svc, \"Original, SVC\")\n",
    "\n",
    "calc_and_add_significance(predictions_dt_unb,dt_predictions, y_test, \"Original, DT\")\n",
    "calc_and_add_significance(predictions_svc_unb,svc_predictions, y_test_svc, \"Original, SVC\")\n",
    "\n",
    "print(\"Best DT estimator: \", search_dtc.best_estimator_)\n",
    "print(\"Best SVC estimator: \", search_svc.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train on SMOTE\n",
    "sampled_dtc_x, sampled_dtc_y = smote_technique(X_train,y_train)\n",
    "sampled_svc_x, sampled_svc_y = smote_technique(X_train_svc,y_train_svc)\n",
    "\n",
    "dt_classifier = search_dtc.fit(sampled_dtc_x, sampled_dtc_y).best_estimator_\n",
    "svc_classifier = search_svc.fit(sampled_svc_x, sampled_svc_y).best_estimator_\n",
    "\n",
    "dt_predictions, dt_probs_preds = get_preds_and_probs(dt_classifier, X_test)\n",
    "svc_predictions, svc_probs_preds = get_preds_and_probs(svc_classifier, X_test_svc)\n",
    "\n",
    "calc_and_add_classifier_results(dt_predictions, dt_probs_preds, y_test, \"SMOTE, DT\")\n",
    "calc_and_add_classifier_results(svc_predictions, svc_probs_preds, y_test_svc, \"SMOTE, SVC\")\n",
    "\n",
    "calc_and_add_significance(predictions_dt_unb,dt_predictions, y_test, \"SMOTE, DT\")\n",
    "calc_and_add_significance(predictions_svc_unb,svc_predictions, y_test_svc, \"SMOTE, SVC\")\n",
    "\n",
    "print(\"Best DT estimator: \", search_dtc.best_estimator_)\n",
    "print(\"Best SVC estimator: \", search_svc.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train on random oversampling\n",
    "sampled_dtc_x, sampled_dtc_y = random_oversampling_technique(X_train,y_train)\n",
    "sampled_svc_x, sampled_svc_y = random_oversampling_technique(X_train_svc,y_train_svc)\n",
    "\n",
    "dt_classifier = search_dtc.fit(sampled_dtc_x, sampled_dtc_y)\n",
    "svc_classifier = search_svc.fit(sampled_svc_x, sampled_svc_y)\n",
    "\n",
    "dt_predictions, dt_probs_preds = get_preds_and_probs(dt_classifier, X_test)\n",
    "svc_predictions, svc_probs_preds = get_preds_and_probs(svc_classifier, X_test_svc)\n",
    "\n",
    "calc_and_add_classifier_results(dt_predictions, dt_probs_preds, y_test, \"Rand_Over, DT\")\n",
    "calc_and_add_classifier_results(svc_predictions, svc_probs_preds, y_test_svc, \"Rand_Over, SVC\")\n",
    "\n",
    "calc_and_add_significance(predictions_dt_unb,dt_predictions, y_test, \"Rand_Over, DT\")\n",
    "calc_and_add_significance(predictions_svc_unb,svc_predictions, y_test_svc, \"Rand_Over, SVC\")\n",
    "\n",
    "print(\"Best DT estimator: \", search_dtc.best_estimator_)\n",
    "print(\"Best SVC estimator: \", search_svc.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train on random undersampling\n",
    "sampled_dtc_x, sampled_dtc_y = random_undersampling_technique(X_train,y_train)\n",
    "sampled_svc_x, sampled_svc_y = random_undersampling_technique(X_train_svc,y_train_svc)\n",
    "\n",
    "dt_classifier = search_dtc.fit(sampled_dtc_x, sampled_dtc_y)\n",
    "svc_classifier = search_svc.fit(sampled_svc_x, sampled_svc_y)\n",
    "\n",
    "dt_predictions, dt_probs_preds = get_preds_and_probs(dt_classifier, X_test)\n",
    "svc_predictions, svc_probs_preds = get_preds_and_probs(svc_classifier, X_test_svc)\n",
    "\n",
    "calc_and_add_classifier_results(dt_predictions, dt_probs_preds, y_test, \"Rand_Under, DT\")\n",
    "calc_and_add_classifier_results(svc_predictions, svc_probs_preds, y_test_svc, \"Rand_Under, SVC\")\n",
    "\n",
    "calc_and_add_significance(predictions_dt_unb,dt_predictions, y_test, \"Rand_Under, DT\")\n",
    "calc_and_add_significance(predictions_svc_unb,svc_predictions, y_test_svc, \"Rand_Under, SVC\")\n",
    "\n",
    "print(\"Best DT estimator: \", search_dtc.best_estimator_)\n",
    "print(\"Best SVC estimator: \", search_svc.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#CHANGE, BAGGINBG NO SAMPLING TECHNIQUES\n",
    "svc = SVC(C=1000, gamma=0.001, random_state=12345, probability=True)\n",
    "dtc = DecisionTreeClassifier(min_samples_leaf=10, min_weight_fraction_leaf=0,splitter='random',random_state=12345)\n",
    "\n",
    "dt_predict, dt_proba = UnbalancedBaggerNoSampling.unbalanced_bagger_no_sampling(X_train, y_train, 10,random_seed=RANDOM_SEED)\n",
    "svc_predict, svc_proba = UnbalancedBaggerNoSampling.unbalanced_bagger_no_sampling(X_train_svc, y_train_svc, 10,random_seed=RANDOM_SEED)\n",
    "\n",
    "dt_predictions = dt_predict(X_test)\n",
    "dt_probs_preds = dt_proba(X_test)\n",
    "svc_predictions = svc_predict(X_test_svc)\n",
    "svc_probs_preds = svc_proba(X_test_svc)\n",
    "\n",
    "calc_and_add_classifier_results(dt_predictions, dt_probs_preds, y_test, \"Bagging, DT\")\n",
    "calc_and_add_classifier_results(svc_predictions, svc_probs_preds, y_test_svc, \"Bagging, SVC\")\n",
    "\n",
    "calc_and_add_significance(predictions_dt_unb,dt_predictions, y_test, \"Bagging, DT\")\n",
    "calc_and_add_significance(predictions_svc_unb,svc_predictions, y_test_svc, \"Bagging, SVC\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RESULTS <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "* Plot results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bars(list_of_precision_results, \"Precision\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bars(list_of_recall_results, \"Recall\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bars(list_of_avg_prec, \"Avg. Precision\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_AUPRC_curves(list_of_auprc)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bars(list_of_auc, \"AUC\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_mcnemar_df())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "DAMI2 Project Report Code - Handling unbalanced datasets\n",
     "\n",
     "2020 Jimmy Ljungman\n",
     "\n",
     "This code imports a data set, preprocesses it, run a new sampling technique\n",
     "called 'Unbalanced_bagging' and measure its performance against other sampling techniques\n",
     "such as random oversampling and random undersampling.\n",
     "\n",
     "Unbalanced_bagging will be measured against the other techniques using the metrics AUPRC and time complexity."
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}